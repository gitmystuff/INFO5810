{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Natural Language Processing\n",
    "\n",
    "* https://www.nltk.org/index.html\n",
    "* https://spacy.io/\n",
    "* https://pypi.org/project/wikipedia/ # pie pea eye or python packaging index\n",
    "* https://kgextension.readthedocs.io/en/latest/\n",
    "\n",
    "NLTK Downloads\n",
    "\n",
    "* install nltk: https://pypi.org/project/nltk/\n",
    "* stopwords: https://pythonspot.com/nltk-stop-words/\n",
    "* punkt: https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
    "* wordnet: https://www.tutorialspoint.com/how-to-get-synonyms-antonyms-from-nltk-wordnet-in-python\n",
    "* averaged_perceptron_tagger: https://morioh.com/p/04a148fa2131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads for processing raw text, meanings, pos tagging, and cleaning\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis\n",
    "\n",
    "From https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction :\n",
    "\n",
    "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "> In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "> * tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators\n",
    "> * counting the occurrences of tokens in each document\n",
    "> * normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "> * each individual token occurrence frequency (normalized or not) is treated as a feature\n",
    "> * the vector of all the token frequencies for a given document is considered a multivariate sample\n",
    "> A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus\n",
    "\n",
    "> We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokens\n",
    "\n",
    "Tokens are the total numbers of words in a corpus regardless if they are repeated. Word tokenization splits text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'learning',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'learned',\n",
       " 'so',\n",
       " 'much',\n",
       " ',',\n",
       " 'and',\n",
       " 'hope',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'more',\n",
       " '.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'hope',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'how',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'learns',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                msgs\n",
      "0  I love learning. I have learned so much, and h...\n",
      "1  Learning about beautiful mice was so much fun ...\n"
     ]
    }
   ],
   "source": [
    "# create dataframe from messages\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "msgs = [\n",
    "    'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns',\n",
    "    'Learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'msgs': msgs})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>fun</th>\n",
       "      <th>going</th>\n",
       "      <th>have</th>\n",
       "      <th>hope</th>\n",
       "      <th>how</th>\n",
       "      <th>...</th>\n",
       "      <th>machine</th>\n",
       "      <th>mice</th>\n",
       "      <th>more</th>\n",
       "      <th>much</th>\n",
       "      <th>quiz</th>\n",
       "      <th>so</th>\n",
       "      <th>there</th>\n",
       "      <th>till</th>\n",
       "      <th>to</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  also  and  be  beautiful  fun  going  have  hope  how  ...  machine  \\\n",
       "0      0     1    1   0          0    0      0     1     2    1  ...        1   \n",
       "1      1     0    0   1          1    1      1     0     0    0  ...        0   \n",
       "\n",
       "   mice  more  much  quiz  so  there  till  to  was  \n",
       "0     0     1     1     0   1      0     0   2    0  \n",
       "1     1     0     1     1   1      1     1   1    2  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "matrix = cv.fit_transform(df['msgs'])\n",
    "cv_df = pd.DataFrame(matrix.toarray(), columns=cv.get_feature_names_out())\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming finds the stem of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn\n",
      "learn\n",
      "learn\n",
      "learn\n"
     ]
    }
   ],
   "source": [
    "# demonstrate stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps =PorterStemmer()\n",
    "words= ['learn', 'learned', 'learning', 'learns']\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'learn', '.', 'i', 'have', 'learn', 'so', 'much', ',', 'and', 'hope', 'to', 'learn', 'more', '.', 'i', 'also', 'hope', 'to', 'learn', 'how', 'a', 'machin', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "# demonstrate stemming and tokenization\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps =PorterStemmer()\n",
    "sentence = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
    "words = word_tokenize(sentence)\n",
    "print([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'learn', '.', 'i', 'have', 'learn', 'so', 'much', ',', 'and', 'hope', 'to', 'learn', 'more', '.', 'i', 'also', 'hope', 'to', 'learn', 'how', 'a', 'machin', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "# demonstrate stemming and tokenization\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sb =SnowballStemmer(language='english')\n",
    "sentence = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
    "words = word_tokenize(sentence)\n",
    "print([sb.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization tries to provide context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning => learn (v)\n",
      "about => about (n)\n",
      "beautiful => beautiful (a)\n",
      "mice => mouse (n)\n",
      "was => be (v)\n",
      "so => so (r)\n",
      "much => much (a)\n",
      "fun => fun (n)\n",
      "till => till (n)\n",
      "I => I (n)\n",
      "learned => learn (v)\n",
      "there => there (n)\n",
      "was => be (v)\n",
      "going => go (v)\n",
      "to => to (n)\n",
      "be => be (v)\n",
      "a => a (n)\n",
      "quiz => quiz (n)\n"
     ]
    }
   ],
   "source": [
    "# demonstrate lemmatization\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text = 'learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
    "tokens = word_tokenize(text)\n",
    "lemma_function = WordNetLemmatizer()\n",
    "\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
    "    print(f'{token} => {lemma} ({tag_map[tag[0]]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency Inverse Document Frequency)\n",
    "\n",
    "* Term frequency vs term usefulness\n",
    "* Simple frequency count can be misleading because frequent terms in one document can also be frequent in other documents\n",
    "* TF-IDF is used to score words in context of the document as well as in the context of the corpus, the higher the score the more useful\n",
    "\n",
    "For example, you are wondering what to take for your electives. You want the class to be good but you also want the class to be relevant to your major. It's easy to see that the class can be:\n",
    "1. both good and relevant\n",
    "2. good but not relevant\n",
    "3. relevant but not good\n",
    "4. not good and not relevant\n",
    "\n",
    "In the same way, a term may be:\n",
    "1. frequently used in your corpus and useful in the analysis of the document it is found in\n",
    "2. frequently used in your corpus but useless\n",
    "3. infrequently used in your corpus but useful\n",
    "4. infrequenlty used in your corpus and useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>fun</th>\n",
       "      <th>going</th>\n",
       "      <th>have</th>\n",
       "      <th>hope</th>\n",
       "      <th>how</th>\n",
       "      <th>...</th>\n",
       "      <th>machine</th>\n",
       "      <th>mice</th>\n",
       "      <th>more</th>\n",
       "      <th>much</th>\n",
       "      <th>quiz</th>\n",
       "      <th>so</th>\n",
       "      <th>there</th>\n",
       "      <th>till</th>\n",
       "      <th>to</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223328</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180542</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.180542</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.253745</td>\n",
       "      <td>0.180542</td>\n",
       "      <td>0.50749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about      also       and        be  beautiful       fun     going  \\\n",
       "0  0.000000  0.223328  0.223328  0.000000   0.000000  0.000000  0.000000   \n",
       "1  0.253745  0.000000  0.000000  0.253745   0.253745  0.253745  0.253745   \n",
       "\n",
       "       have      hope       how  ...   machine      mice      more      much  \\\n",
       "0  0.223328  0.446656  0.223328  ...  0.223328  0.000000  0.223328  0.158900   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.253745  0.000000  0.180542   \n",
       "\n",
       "       quiz        so     there      till        to      was  \n",
       "0  0.000000  0.158900  0.000000  0.000000  0.317800  0.00000  \n",
       "1  0.253745  0.180542  0.253745  0.253745  0.180542  0.50749  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf demonstration\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text1 = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
    "text2 = 'learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([text1, text2])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# add words\n",
    "add_stopwords = ['word1', 'word2']\n",
    "stopwords = stopwords.union(add_stopwords)\n",
    "\n",
    "# remove words\n",
    "remove_stopwords = {'word1', 'word2'} \n",
    "stopwords = set([word for word in stopwords if word not in remove_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                msgs\n",
      "0  love learning. learned much, hope learn more. ...\n",
      "1  learning beautiful mice much fun till learned ...\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/54366913/removing-stopwords-from-a-pandas-dataframe\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "msgs = [\n",
    "    'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns',\n",
    "    'Learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'msgs': msgs})\n",
    "stopwords = set(stopwords.words('english')) \n",
    "# df['msgs'] = df['msgs'].str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
    "df['msgs'] = df['msgs'].apply(lambda x: ' '.join([item.lower() for item in x.split() if item.lower() not in stopwords]))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it’s possible to solve some problems starting from only the raw characters, it’s usually better to use linguistic knowledge to add useful information. That’s exactly what spaCy is designed to do: you put in raw text, and get back a Doc object, that comes with a variety of annotations.\n",
    "\n",
    "https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS)\n",
    "\n",
    "* https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell took a while to download. en_core_web_md = 40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_core_web_md is an English pipeline trained on written web text (blogs, news, comments), \n",
    "# that includes vocabulary, syntax, entities, and vectors\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to read about data analysis everyday. I read a book on knowledge discovery last night.\n"
     ]
    }
   ],
   "source": [
    "# create spacy doc with u(nicode) string\n",
    "doc = nlp(u\"I like to read about data analysis everyday. I read a book on knowledge discovery last night.\")\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          PRON     PRP    pronoun, personal\n",
      "like       VERB     VBP    verb, non-3rd person singular present\n",
      "to         PART     TO     infinitival \"to\"\n",
      "read       VERB     VB     verb, base form\n",
      "about      ADP      IN     conjunction, subordinating or preposition\n",
      "data       NOUN     NN     noun, singular or mass\n",
      "analysis   NOUN     NN     noun, singular or mass\n",
      "everyday   NOUN     NN     noun, singular or mass\n",
      ".          PUNCT    .      punctuation mark, sentence closer\n",
      "I          PRON     PRP    pronoun, personal\n",
      "read       VERB     VBD    verb, past tense\n",
      "a          DET      DT     determiner\n",
      "book       NOUN     NN     noun, singular or mass\n",
      "on         ADP      IN     conjunction, subordinating or preposition\n",
      "knowledge  NOUN     NN     noun, singular or mass\n",
      "discovery  PROPN    NNP    noun, proper singular\n",
      "last       ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "night      NOUN     NN     noun, singular or mass\n",
      ".          PUNCT    .      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "* GPE: Geographical Entity\n",
    "* Org: Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denton GPE Countries, cities, states\n",
      "Texas GPE Countries, cities, states\n",
      "last night TIME Times smaller than a day\n",
      "WikiData ORG Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"I read about data analysis in Denton Texas everyday. I read a book on knowledge discovery last night from WikiData.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read about data analysis in Denton Texas everyday.\n",
      "I read a book on knowledge discovery last night from WikiData.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"I read about data analysis in Denton Texas everyday. I read a book on knowledge discovery last night from WikiData.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "banana True 31.620354 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp('dog cat banana afskfsd')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) # oov out-of-vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic modeling helps us \n",
    "\n",
    "* discover hidden, or latent, topics, or themes in documents\n",
    "* summarize documents\n",
    "* search for similar documents\n",
    "* classify documents\n",
    "\n",
    "A document consists of topics and topics consist of words. The same word can be a part of multiple topics and one topic can be part of multiple documents. We can assign probabilities to how relevant a word is in one topic and that probability can be larger or smaller in another topic. The same can be said in the relationship between topics and documents. We can use topics and the words in topics for knowledge discovery without going through the entire document. These latent topics are like clustering, which we’ll cover a little more next week, and since they’re latent, we really don’t know, at first, what the big theme of the topic is so we just say that this group of words belong to topic 1, this group of words to topic 2, and so on. At some point, we might be able to give a topic a name, but it’s not necessary for our purposes. We want to find documents with similar topics because those topics have the same words, or key words, we’re looking for. In a sense, we can start annotating our documents by topics to optimize our searching.\n",
    "\n",
    "\n",
    "### Latent Dirichlet Allocation\n",
    "\n",
    "In natural language processing, Latent Dirichlet Allocation (LDA) is a generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar. LDA is an example of a topic model. In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "* Documents with similar topics use similar groups of words\n",
    "* Topics can be discovered (latent topics) by finding words that frequently occur together in a document\n",
    "* It's up to the user to label the topics based on the words within that topic (topic with the words Titanic and Carpathia might be Ship Tragedies)\n",
    "* LDA represents documents as probabilities of topics which consists of probabilities of words\n",
    "* LDA requires us to select the number of topics (K), the topics are just numbers\n",
    "* Then we randomly assign words in a document to a K topic\n",
    "* Then we find the proportion of words assigned to the topic p(topic t | document d)\n",
    "* We also find p(word w | topic t)\n",
    "* Then we reassign the word to a new topic with p(topic t | document d) * p(word w | topic t)\n",
    "* This is the probability that the topic generated the word\n",
    "* This is done a large number of times till words to topics are acceptable (clustering)\n",
    "\n",
    "https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n",
    "\n",
    "https://highdemandskills.com/topic-modeling-intuitive/\n",
    "\n",
    "### Non-Negative Matrix Factorization\n",
    "\n",
    "Non-Negative Matrix Factorization is a statistical method that helps us to reduce the dimension of the input corpora or corpora. Internally, it uses the factor analysis method to give comparatively less weightage to the words that are having less coherence.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/ \n",
    "\n",
    "* Performs dimensionality reduction and clustering\n",
    "* Used with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aue (Elbe)</td>\n",
       "      <td>The Aue is a river in northern Germany in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marco Camargo</td>\n",
       "      <td>Marco Antonio Camargo González (born May 8, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Girl Guides Association of Zambia</td>\n",
       "      <td>The Girl Guides Association of Zambia is the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asiedu</td>\n",
       "      <td>Asiedu is both a surname and a given name. Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carol Morley</td>\n",
       "      <td>Carol Anne Morley (born 14 January 1966) is an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                         Aue (Elbe)   \n",
       "1                      Marco Camargo   \n",
       "2  Girl Guides Association of Zambia   \n",
       "3                             Asiedu   \n",
       "4                       Carol Morley   \n",
       "\n",
       "                                             content  \n",
       "0  The Aue is a river in northern Germany in the ...  \n",
       "1  Marco Antonio Camargo González (born May 8, 19...  \n",
       "2  The Girl Guides Association of Zambia is the n...  \n",
       "3  Asiedu is both a surname and a given name. Not...  \n",
       "4  Carol Anne Morley (born 14 January 1966) is an...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://kleiber.me/blog/2017/07/22/tutorial-lda-wikipedia/\n",
    "import pandas as pd\n",
    "import random\n",
    "import wikipedia\n",
    "\n",
    "titles = wikipedia.random(5)\n",
    "\n",
    "content = []\n",
    "for title in titles:\n",
    "    # disambiguous error fix\n",
    "    try:\n",
    "        content.append([title, wikipedia.page(title).content])\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        s = random.choice(e.options)\n",
    "        content.append([title, wikipedia.page(s).content])\n",
    "\n",
    "df = pd.DataFrame(content, columns=['title', 'content'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 top words: ['km', 'lower', 'saxony', 'horneburg', 'river']\n",
      "Topic 1 top words: ['girl', 'butterfly', 'association', 'ghanaian', 'asiedu']\n",
      "Topic 2 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 3 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 4 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 5 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 6 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 7 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 8 top words: ['external', '2018', '1966', 'features', 'references']\n",
      "Topic 9 top words: ['life', 'years', 'short', 'film', 'morley']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectors = vectorizer.fit_transform(df['content'].values.astype('U'))\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "model.fit(vectors)\n",
    "\n",
    "for index, topic in enumerate(model.components_):\n",
    "    print(f'Topic {index} top words: {[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aue (Elbe)</td>\n",
       "      <td>The Aue is a river in northern Germany in the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marco Camargo</td>\n",
       "      <td>Marco Antonio Camargo González (born May 8, 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Girl Guides Association of Zambia</td>\n",
       "      <td>The Girl Guides Association of Zambia is the n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asiedu</td>\n",
       "      <td>Asiedu is both a surname and a given name. Not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carol Morley</td>\n",
       "      <td>Carol Anne Morley (born 14 January 1966) is an...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                         Aue (Elbe)   \n",
       "1                      Marco Camargo   \n",
       "2  Girl Guides Association of Zambia   \n",
       "3                             Asiedu   \n",
       "4                       Carol Morley   \n",
       "\n",
       "                                             content  topic  \n",
       "0  The Aue is a river in northern Germany in the ...      0  \n",
       "1  Marco Antonio Camargo González (born May 8, 19...      1  \n",
       "2  The Girl Guides Association of Zambia is the n...      1  \n",
       "3  Asiedu is both a surname and a given name. Not...      1  \n",
       "4  Carol Anne Morley (born 14 January 1966) is an...      9  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results = model.transform(vectors)\n",
    "df['topic'] = topic_results.argmax(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF (Non-Negative Matrix Factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>16mm</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>1924</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>written</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yirenkyi</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zambia</th>\n",
       "      <th>zawe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331322</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.013517</td>\n",
       "      <td>0.050261</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067015</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.040550</td>\n",
       "      <td>0.201044</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.050261</td>\n",
       "      <td>0.013517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        100        11        12        14        15        16      16mm  \\\n",
       "0  0.000000  0.000000  0.077198  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.214946  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.016754  0.013517  0.050261  0.033507  0.033507  0.033507   \n",
       "\n",
       "         18        19      1924  ...     world   written     wrote      year  \\\n",
       "0  0.000000  0.077198  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.086708  0.000000  ...  0.086708  0.000000  0.000000  0.086708   \n",
       "2  0.000000  0.000000  0.110441  ...  0.089103  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.016754  0.000000  0.000000  ...  0.000000  0.067015  0.016754  0.040550   \n",
       "\n",
       "      years  yirenkyi     young     youth    zambia      zawe  \n",
       "0  0.000000   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000   0.00000  0.000000  0.086708  0.000000  0.000000  \n",
       "2  0.000000   0.00000  0.000000  0.000000  0.331322  0.000000  \n",
       "3  0.000000   0.10309  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.201044   0.00000  0.050261  0.013517  0.000000  0.016754  \n",
       "\n",
       "[5 rows x 532 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectors = vectorizer.fit_transform(df['content'].values.astype('U'))\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 532)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1706.05084.pdf\n",
    "\n",
    "Figure 1: Illustration of NMF model for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 top words: ['life', 'years', 'short', 'film', 'morley']\n",
      "Topic 1 top words: ['km', 'mi', 'aue', 'horneburg', 'river']\n",
      "Topic 2 top words: ['organization', 'zambia', 'guides', 'girl', 'association']\n",
      "Topic 3 top words: ['scouts', 'guides', 'zambia', 'girl', 'association']\n",
      "Topic 4 top words: ['ecuador', 'fina', 'camargo', 'olympics', 'butterfly']\n",
      "Topic 5 top words: ['2009', 'girl', 'guides', 'zambia', 'association']\n",
      "Topic 6 top words: ['ghanaian', '2018', 'born', 'asiedu', 'people']\n",
      "Topic 7 top words: ['documentary', 'years', 'short', 'morley', 'film']\n",
      "Topic 8 top words: ['surname', 'given', 'politician', 'ghanaian', 'asiedu']\n",
      "Topic 9 top words: ['200', 'summer', '100', 'olympics', 'butterfly']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(init='random', n_components=10, random_state=42)\n",
    "model.fit(vectors)\n",
    "\n",
    "for index, topic in enumerate(model.components_):\n",
    "    print(f'Topic {index} top words: {[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cliff\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aue (Elbe)</td>\n",
       "      <td>The Aue is a river in northern Germany in the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marco Camargo</td>\n",
       "      <td>Marco Antonio Camargo González (born May 8, 19...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Girl Guides Association of Zambia</td>\n",
       "      <td>The Girl Guides Association of Zambia is the n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asiedu</td>\n",
       "      <td>Asiedu is both a surname and a given name. Not...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carol Morley</td>\n",
       "      <td>Carol Anne Morley (born 14 January 1966) is an...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                         Aue (Elbe)   \n",
       "1                      Marco Camargo   \n",
       "2  Girl Guides Association of Zambia   \n",
       "3                             Asiedu   \n",
       "4                       Carol Morley   \n",
       "\n",
       "                                             content  topic  \n",
       "0  The Aue is a river in northern Germany in the ...      1  \n",
       "1  Marco Antonio Camargo González (born May 8, 19...      9  \n",
       "2  The Girl Guides Association of Zambia is the n...      2  \n",
       "3  Asiedu is both a surname and a given name. Not...      8  \n",
       "4  Carol Anne Morley (born 14 January 1966) is an...      7  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results = model.transform(vectors)\n",
    "df['topic'] = topic_results.argmax(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Girl Guides Association of Zambia',\n",
       " 'List of World Association of Girl Guides and Girl Scouts members',\n",
       " 'Scouting and Guiding in Zambia',\n",
       " 'List of World Organization of the Scout Movement members',\n",
       " 'Zambia']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = model.components_[2]\n",
    "keywords = ' '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])\n",
    "wikipedia.search(keywords, results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "\n",
    "If you intend to do any scraping projects or automated requests, consider alternatives such as Pywikipediabot or MediaWiki API, which has other superior features.\n",
    "\n",
    "* wikipedia.search('keywords', results=2)\n",
    "* wikipedia.suggest('keyword')\n",
    "* wikipedia.summary('keywords', sentences=2)\n",
    "* wikipedia.page('keywords')\n",
    "* wikipedia.page('keywords').content\n",
    "* wikipedia.page('keywords').references\n",
    "* wikipedia.page('keywords').title\n",
    "* wikipedia.page('keywords').url\n",
    "* wikipedia.page('keywords').categories\n",
    "* wikipedia.page('keywords').content\n",
    "* wikipedia.page('keywords').links\n",
    "* wikipedia.geosearch(33.2075, 97.1526)\n",
    "* wikipedia.set_lang('hi')\n",
    "* wikipedia.languages()\n",
    "* wikipedia.page('keywords').images[0]\n",
    "* wikipedia.page('keywords').html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARQL-dataframe\n",
    "\n",
    "* The wikipedia api\n",
    "* https://pypi.org/project/SPARQLWrapper/\n",
    "* https://sparqlwrapper.readthedocs.io/en/latest/main.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sparql-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>ship</th>\n",
       "      <th>owner</th>\n",
       "      <th>status</th>\n",
       "      <th>port</th>\n",
       "      <th>route</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SS Arabic</td>\n",
       "      <td>http://dbpedia.org/resource/SS_Arabic_(1881)</td>\n",
       "      <td>http://dbpedia.org/resource/White_Star_Line</td>\n",
       "      <td>Broken up August 1901</td>\n",
       "      <td>http://dbpedia.org/resource/Rotterdam</td>\n",
       "      <td>*Liverpool-New York \\n*San Francisco-Hong Kong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SS Spaarndam</td>\n",
       "      <td>http://dbpedia.org/resource/SS_Arabic_(1881)</td>\n",
       "      <td>http://dbpedia.org/resource/White_Star_Line</td>\n",
       "      <td>Broken up August 1901</td>\n",
       "      <td>http://dbpedia.org/resource/Rotterdam</td>\n",
       "      <td>*Liverpool-New York \\n*San Francisco-Hong Kong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SS Arabic</td>\n",
       "      <td>http://dbpedia.org/resource/SS_Arabic_(1881)</td>\n",
       "      <td>http://dbpedia.org/resource/White_Star_Line</td>\n",
       "      <td>Sold to theHolland America Linein February 1890</td>\n",
       "      <td>http://dbpedia.org/resource/Rotterdam</td>\n",
       "      <td>*Liverpool-New York \\n*San Francisco-Hong Kong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SS Spaarndam</td>\n",
       "      <td>http://dbpedia.org/resource/SS_Arabic_(1881)</td>\n",
       "      <td>http://dbpedia.org/resource/White_Star_Line</td>\n",
       "      <td>Sold to theHolland America Linein February 1890</td>\n",
       "      <td>http://dbpedia.org/resource/Rotterdam</td>\n",
       "      <td>*Liverpool-New York \\n*San Francisco-Hong Kong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SS Arabic</td>\n",
       "      <td>http://dbpedia.org/resource/SS_Arabic_(1881)</td>\n",
       "      <td>http://dbpedia.org/resource/White_Star_Line</td>\n",
       "      <td>Broken up August 1901</td>\n",
       "      <td>http://dbpedia.org/resource/Liverpool</td>\n",
       "      <td>*Liverpool-New York \\n*San Francisco-Hong Kong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                          ship  \\\n",
       "0     SS Arabic  http://dbpedia.org/resource/SS_Arabic_(1881)   \n",
       "1  SS Spaarndam  http://dbpedia.org/resource/SS_Arabic_(1881)   \n",
       "2     SS Arabic  http://dbpedia.org/resource/SS_Arabic_(1881)   \n",
       "3  SS Spaarndam  http://dbpedia.org/resource/SS_Arabic_(1881)   \n",
       "4     SS Arabic  http://dbpedia.org/resource/SS_Arabic_(1881)   \n",
       "\n",
       "                                         owner  \\\n",
       "0  http://dbpedia.org/resource/White_Star_Line   \n",
       "1  http://dbpedia.org/resource/White_Star_Line   \n",
       "2  http://dbpedia.org/resource/White_Star_Line   \n",
       "3  http://dbpedia.org/resource/White_Star_Line   \n",
       "4  http://dbpedia.org/resource/White_Star_Line   \n",
       "\n",
       "                                            status  \\\n",
       "0                            Broken up August 1901   \n",
       "1                            Broken up August 1901   \n",
       "2  Sold to theHolland America Linein February 1890   \n",
       "3  Sold to theHolland America Linein February 1890   \n",
       "4                            Broken up August 1901   \n",
       "\n",
       "                                    port  \\\n",
       "0  http://dbpedia.org/resource/Rotterdam   \n",
       "1  http://dbpedia.org/resource/Rotterdam   \n",
       "2  http://dbpedia.org/resource/Rotterdam   \n",
       "3  http://dbpedia.org/resource/Rotterdam   \n",
       "4  http://dbpedia.org/resource/Liverpool   \n",
       "\n",
       "                                               route  \n",
       "0  *Liverpool-New York \\n*San Francisco-Hong Kong...  \n",
       "1  *Liverpool-New York \\n*San Francisco-Hong Kong...  \n",
       "2  *Liverpool-New York \\n*San Francisco-Hong Kong...  \n",
       "3  *Liverpool-New York \\n*San Francisco-Hong Kong...  \n",
       "4  *Liverpool-New York \\n*San Francisco-Hong Kong...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparql_dataframe\n",
    "\n",
    "endpoint = \"http://dbpedia.org/sparql\"\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT ?label ?ship ?owner ?status ?port ?route\n",
    "WHERE\n",
    "{\n",
    " ?ship dbp:shipName ?label .\n",
    " ?ship rdf:type dbo:Ship .\n",
    " ?ship dbo:owner ?owner FILTER ( ?owner = dbr:White_Star_Line ) .\n",
    " ?ship dbo:status ?status .\n",
    " ?ship dbp:shipRegistry ?port .\n",
    " ?ship dbp:shipRoute ?route .\n",
    "}\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "df = sparql_dataframe.get(endpoint, q)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph Extension\n",
    "\n",
    "https://colab.research.google.com/github/om-hb/kgextension/blob/master/examples/book_genre_prediction.ipynb\n",
    "\n",
    "The kgextension package allows to access and use Linked Open Data to augment existing datasets. It enables to incorporate knowledge graph information in pandas. DataFrames and can be used within the scikit-learn pipeline.\n",
    "\n",
    "Its functionality includes:\n",
    "\n",
    "* Linking datasets to any Linked Open Data (LOD) Source such as DBpedia, WikiData or the EU Open Data Portal\n",
    "* Generation of new features from the LOD Sources\n",
    "* Hierarchy-based feature selection algorithms\n",
    "* Data Integration of features from different sources\n",
    "\n",
    "https://kgextension.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kgextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>release_year</th>\n",
       "      <th>age_certification</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>seasons</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>tmdb_popularity</th>\n",
       "      <th>tmdb_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ts300399</td>\n",
       "      <td>Five Came Back: The Reference Films</td>\n",
       "      <td>SHOW</td>\n",
       "      <td>This collection includes 12 World War II-era p...</td>\n",
       "      <td>1945</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>48</td>\n",
       "      <td>['documentation']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tm84618</td>\n",
       "      <td>Taxi Driver</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>A mentally unstable Vietnam War veteran works ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>R</td>\n",
       "      <td>113</td>\n",
       "      <td>['crime', 'drama']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0075314</td>\n",
       "      <td>8.3</td>\n",
       "      <td>795222.0</td>\n",
       "      <td>27.612</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tm127384</td>\n",
       "      <td>Monty Python and the Holy Grail</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>King Arthur, accompanied by his squire, recrui...</td>\n",
       "      <td>1975</td>\n",
       "      <td>PG</td>\n",
       "      <td>91</td>\n",
       "      <td>['comedy', 'fantasy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0071853</td>\n",
       "      <td>8.2</td>\n",
       "      <td>530877.0</td>\n",
       "      <td>18.216</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tm70993</td>\n",
       "      <td>Life of Brian</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>Brian Cohen is an average young Jewish man, bu...</td>\n",
       "      <td>1979</td>\n",
       "      <td>R</td>\n",
       "      <td>94</td>\n",
       "      <td>['comedy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0079470</td>\n",
       "      <td>8.0</td>\n",
       "      <td>392419.0</td>\n",
       "      <td>17.505</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tm190788</td>\n",
       "      <td>The Exorcist</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>12-year-old Regan MacNeil begins to adapt an e...</td>\n",
       "      <td>1973</td>\n",
       "      <td>R</td>\n",
       "      <td>133</td>\n",
       "      <td>['horror']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0070047</td>\n",
       "      <td>8.1</td>\n",
       "      <td>391942.0</td>\n",
       "      <td>95.337</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                title   type  \\\n",
       "0  ts300399  Five Came Back: The Reference Films   SHOW   \n",
       "1   tm84618                          Taxi Driver  MOVIE   \n",
       "2  tm127384      Monty Python and the Holy Grail  MOVIE   \n",
       "3   tm70993                        Life of Brian  MOVIE   \n",
       "4  tm190788                         The Exorcist  MOVIE   \n",
       "\n",
       "                                         description  release_year  \\\n",
       "0  This collection includes 12 World War II-era p...          1945   \n",
       "1  A mentally unstable Vietnam War veteran works ...          1976   \n",
       "2  King Arthur, accompanied by his squire, recrui...          1975   \n",
       "3  Brian Cohen is an average young Jewish man, bu...          1979   \n",
       "4  12-year-old Regan MacNeil begins to adapt an e...          1973   \n",
       "\n",
       "  age_certification  runtime                 genres production_countries  \\\n",
       "0             TV-MA       48      ['documentation']               ['US']   \n",
       "1                 R      113     ['crime', 'drama']               ['US']   \n",
       "2                PG       91  ['comedy', 'fantasy']               ['GB']   \n",
       "3                 R       94             ['comedy']               ['GB']   \n",
       "4                 R      133             ['horror']               ['US']   \n",
       "\n",
       "   seasons    imdb_id  imdb_score  imdb_votes  tmdb_popularity  tmdb_score  \n",
       "0      1.0        NaN         NaN         NaN            0.600         NaN  \n",
       "1      NaN  tt0075314         8.3    795222.0           27.612         8.2  \n",
       "2      NaN  tt0071853         8.2    530877.0           18.216         7.8  \n",
       "3      NaN  tt0079470         8.0    392419.0           17.505         7.8  \n",
       "4      NaN  tt0070047         8.1    391942.0           95.337         7.7  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "import pandas as pd\n",
    "\n",
    "titles = pd.read_csv('titles.csv')\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193111e7ec544c05ba78c9f5fc84e286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DBpedia Lookup Linker: Querying DLL:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>release_year</th>\n",
       "      <th>age_certification</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>seasons</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>tmdb_popularity</th>\n",
       "      <th>tmdb_score</th>\n",
       "      <th>new_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ts300399</td>\n",
       "      <td>Five Came Back: The Reference Films</td>\n",
       "      <td>SHOW</td>\n",
       "      <td>This collection includes 12 World War II-era p...</td>\n",
       "      <td>1945</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>48</td>\n",
       "      <td>['documentation']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tm84618</td>\n",
       "      <td>Taxi Driver</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>A mentally unstable Vietnam War veteran works ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>R</td>\n",
       "      <td>113</td>\n",
       "      <td>['crime', 'drama']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0075314</td>\n",
       "      <td>8.3</td>\n",
       "      <td>795222.0</td>\n",
       "      <td>27.612</td>\n",
       "      <td>8.2</td>\n",
       "      <td>http://dbpedia.org/resource/Taxi_Driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tm127384</td>\n",
       "      <td>Monty Python and the Holy Grail</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>King Arthur, accompanied by his squire, recrui...</td>\n",
       "      <td>1975</td>\n",
       "      <td>PG</td>\n",
       "      <td>91</td>\n",
       "      <td>['comedy', 'fantasy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0071853</td>\n",
       "      <td>8.2</td>\n",
       "      <td>530877.0</td>\n",
       "      <td>18.216</td>\n",
       "      <td>7.8</td>\n",
       "      <td>http://dbpedia.org/resource/Monty_Python_and_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tm70993</td>\n",
       "      <td>Life of Brian</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>Brian Cohen is an average young Jewish man, bu...</td>\n",
       "      <td>1979</td>\n",
       "      <td>R</td>\n",
       "      <td>94</td>\n",
       "      <td>['comedy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0079470</td>\n",
       "      <td>8.0</td>\n",
       "      <td>392419.0</td>\n",
       "      <td>17.505</td>\n",
       "      <td>7.8</td>\n",
       "      <td>http://dbpedia.org/resource/Monty_Python's_Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tm190788</td>\n",
       "      <td>The Exorcist</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>12-year-old Regan MacNeil begins to adapt an e...</td>\n",
       "      <td>1973</td>\n",
       "      <td>R</td>\n",
       "      <td>133</td>\n",
       "      <td>['horror']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0070047</td>\n",
       "      <td>8.1</td>\n",
       "      <td>391942.0</td>\n",
       "      <td>95.337</td>\n",
       "      <td>7.7</td>\n",
       "      <td>http://dbpedia.org/resource/The_Exorcist_(film)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                title   type  \\\n",
       "0  ts300399  Five Came Back: The Reference Films   SHOW   \n",
       "1   tm84618                          Taxi Driver  MOVIE   \n",
       "2  tm127384      Monty Python and the Holy Grail  MOVIE   \n",
       "3   tm70993                        Life of Brian  MOVIE   \n",
       "4  tm190788                         The Exorcist  MOVIE   \n",
       "\n",
       "                                         description  release_year  \\\n",
       "0  This collection includes 12 World War II-era p...          1945   \n",
       "1  A mentally unstable Vietnam War veteran works ...          1976   \n",
       "2  King Arthur, accompanied by his squire, recrui...          1975   \n",
       "3  Brian Cohen is an average young Jewish man, bu...          1979   \n",
       "4  12-year-old Regan MacNeil begins to adapt an e...          1973   \n",
       "\n",
       "  age_certification  runtime                 genres production_countries  \\\n",
       "0             TV-MA       48      ['documentation']               ['US']   \n",
       "1                 R      113     ['crime', 'drama']               ['US']   \n",
       "2                PG       91  ['comedy', 'fantasy']               ['GB']   \n",
       "3                 R       94             ['comedy']               ['GB']   \n",
       "4                 R      133             ['horror']               ['US']   \n",
       "\n",
       "   seasons    imdb_id  imdb_score  imdb_votes  tmdb_popularity  tmdb_score  \\\n",
       "0      1.0        NaN         NaN         NaN            0.600         NaN   \n",
       "1      NaN  tt0075314         8.3    795222.0           27.612         8.2   \n",
       "2      NaN  tt0071853         8.2    530877.0           18.216         7.8   \n",
       "3      NaN  tt0079470         8.0    392419.0           17.505         7.8   \n",
       "4      NaN  tt0070047         8.1    391942.0           95.337         7.7   \n",
       "\n",
       "                                            new_link  \n",
       "0                                                NaN  \n",
       "1            http://dbpedia.org/resource/Taxi_Driver  \n",
       "2  http://dbpedia.org/resource/Monty_Python_and_t...  \n",
       "3  http://dbpedia.org/resource/Monty_Python's_Lif...  \n",
       "4    http://dbpedia.org/resource/The_Exorcist_(film)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kgextension.linking_sklearn import DbpediaLookupLinker\n",
    "\n",
    "linker = DbpediaLookupLinker(column='title')\n",
    "df_enhanced = linker.fit_transform(titles.head())\n",
    "df_enhanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df094c0bc9f4465cbcfda499d76b8786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Column:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>release_year</th>\n",
       "      <th>age_certification</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>seasons</th>\n",
       "      <th>...</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_about_telekinesis</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_shot_in_Washington,_D.C.</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_set_in_Washington,_D.C.</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Demons_in_film</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_set_in_Iraq</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_shot_in_Iraq</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_featuring_a_Best_Supporting_Actress_Golden_Globe-winning_performance</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Films_scored_by_Jack_Nitzsche</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Rating_controversies_in_film</th>\n",
       "      <th>new_link_in_boolean_http://dbpedia.org/resource/Category:Supernatural_drama_films</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ts300399</td>\n",
       "      <td>Five Came Back: The Reference Films</td>\n",
       "      <td>SHOW</td>\n",
       "      <td>This collection includes 12 World War II-era p...</td>\n",
       "      <td>1945</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>48</td>\n",
       "      <td>['documentation']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tm84618</td>\n",
       "      <td>Taxi Driver</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>A mentally unstable Vietnam War veteran works ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>R</td>\n",
       "      <td>113</td>\n",
       "      <td>['crime', 'drama']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tm127384</td>\n",
       "      <td>Monty Python and the Holy Grail</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>King Arthur, accompanied by his squire, recrui...</td>\n",
       "      <td>1975</td>\n",
       "      <td>PG</td>\n",
       "      <td>91</td>\n",
       "      <td>['comedy', 'fantasy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tm70993</td>\n",
       "      <td>Life of Brian</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>Brian Cohen is an average young Jewish man, bu...</td>\n",
       "      <td>1979</td>\n",
       "      <td>R</td>\n",
       "      <td>94</td>\n",
       "      <td>['comedy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tm190788</td>\n",
       "      <td>The Exorcist</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>12-year-old Regan MacNeil begins to adapt an e...</td>\n",
       "      <td>1973</td>\n",
       "      <td>R</td>\n",
       "      <td>133</td>\n",
       "      <td>['horror']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                title   type  \\\n",
       "0  ts300399  Five Came Back: The Reference Films   SHOW   \n",
       "1   tm84618                          Taxi Driver  MOVIE   \n",
       "2  tm127384      Monty Python and the Holy Grail  MOVIE   \n",
       "3   tm70993                        Life of Brian  MOVIE   \n",
       "4  tm190788                         The Exorcist  MOVIE   \n",
       "\n",
       "                                         description  release_year  \\\n",
       "0  This collection includes 12 World War II-era p...          1945   \n",
       "1  A mentally unstable Vietnam War veteran works ...          1976   \n",
       "2  King Arthur, accompanied by his squire, recrui...          1975   \n",
       "3  Brian Cohen is an average young Jewish man, bu...          1979   \n",
       "4  12-year-old Regan MacNeil begins to adapt an e...          1973   \n",
       "\n",
       "  age_certification  runtime                 genres production_countries  \\\n",
       "0             TV-MA       48      ['documentation']               ['US']   \n",
       "1                 R      113     ['crime', 'drama']               ['US']   \n",
       "2                PG       91  ['comedy', 'fantasy']               ['GB']   \n",
       "3                 R       94             ['comedy']               ['GB']   \n",
       "4                 R      133             ['horror']               ['US']   \n",
       "\n",
       "   seasons  ...  \\\n",
       "0      1.0  ...   \n",
       "1      NaN  ...   \n",
       "2      NaN  ...   \n",
       "3      NaN  ...   \n",
       "4      NaN  ...   \n",
       "\n",
       "  new_link_in_boolean_http://dbpedia.org/resource/Category:Films_about_telekinesis  \\\n",
       "0                                                NaN                                 \n",
       "1                                              False                                 \n",
       "2                                              False                                 \n",
       "3                                              False                                 \n",
       "4                                               True                                 \n",
       "\n",
       "   new_link_in_boolean_http://dbpedia.org/resource/Category:Films_shot_in_Washington,_D.C.  \\\n",
       "0                                                NaN                                         \n",
       "1                                              False                                         \n",
       "2                                              False                                         \n",
       "3                                              False                                         \n",
       "4                                               True                                         \n",
       "\n",
       "   new_link_in_boolean_http://dbpedia.org/resource/Category:Films_set_in_Washington,_D.C.  \\\n",
       "0                                                NaN                                        \n",
       "1                                              False                                        \n",
       "2                                              False                                        \n",
       "3                                              False                                        \n",
       "4                                               True                                        \n",
       "\n",
       "   new_link_in_boolean_http://dbpedia.org/resource/Category:Demons_in_film  \\\n",
       "0                                                NaN                         \n",
       "1                                              False                         \n",
       "2                                              False                         \n",
       "3                                              False                         \n",
       "4                                               True                         \n",
       "\n",
       "   new_link_in_boolean_http://dbpedia.org/resource/Category:Films_set_in_Iraq  \\\n",
       "0                                                NaN                            \n",
       "1                                              False                            \n",
       "2                                              False                            \n",
       "3                                              False                            \n",
       "4                                               True                            \n",
       "\n",
       "  new_link_in_boolean_http://dbpedia.org/resource/Category:Films_shot_in_Iraq  \\\n",
       "0                                                NaN                            \n",
       "1                                              False                            \n",
       "2                                              False                            \n",
       "3                                              False                            \n",
       "4                                               True                            \n",
       "\n",
       "  new_link_in_boolean_http://dbpedia.org/resource/Category:Films_featuring_a_Best_Supporting_Actress_Golden_Globe-winning_performance  \\\n",
       "0                                                NaN                                                                                    \n",
       "1                                              False                                                                                    \n",
       "2                                              False                                                                                    \n",
       "3                                              False                                                                                    \n",
       "4                                               True                                                                                    \n",
       "\n",
       "  new_link_in_boolean_http://dbpedia.org/resource/Category:Films_scored_by_Jack_Nitzsche  \\\n",
       "0                                                NaN                                       \n",
       "1                                              False                                       \n",
       "2                                              False                                       \n",
       "3                                              False                                       \n",
       "4                                               True                                       \n",
       "\n",
       "  new_link_in_boolean_http://dbpedia.org/resource/Category:Rating_controversies_in_film  \\\n",
       "0                                                NaN                                      \n",
       "1                                              False                                      \n",
       "2                                              False                                      \n",
       "3                                              False                                      \n",
       "4                                               True                                      \n",
       "\n",
       "  new_link_in_boolean_http://dbpedia.org/resource/Category:Supernatural_drama_films  \n",
       "0                                                NaN                                 \n",
       "1                                              False                                 \n",
       "2                                              False                                 \n",
       "3                                              False                                 \n",
       "4                                               True                                 \n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://kgextension.readthedocs.io/en/latest/source/usage_generators.html#specific-relation-generator\n",
    "from kgextension.generator_sklearn import SpecificRelationGenerator\n",
    "\n",
    "generator = SpecificRelationGenerator(columns=['new_link'], direct_relation='http://purl.org/dc/terms/subject')\n",
    "df_enhanced = generator.fit_transform(df_enhanced)\n",
    "df_enhanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a53f0dfafa497aa6d359298bdd95cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DBpedia Lookup Linker: Querying DLL:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b15b5ce25a4580abbf23cdf92f03b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Column:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>release_year</th>\n",
       "      <th>age_certification</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>seasons</th>\n",
       "      <th>...</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/starring</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/studio</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/title</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/totalWidth</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/type</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/width</th>\n",
       "      <th>new_link_data_http://dbpedia.org/property/writers</th>\n",
       "      <th>new_link_data_http://www.w3.org/2000/01/rdf-schema#comment</th>\n",
       "      <th>new_link_data_http://www.w3.org/2000/01/rdf-schema#label</th>\n",
       "      <th>new_link_data_http://xmlns.com/foaf/0.1/name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ts300399</td>\n",
       "      <td>Five Came Back: The Reference Films</td>\n",
       "      <td>SHOW</td>\n",
       "      <td>This collection includes 12 World War II-era p...</td>\n",
       "      <td>1945</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>48</td>\n",
       "      <td>['documentation']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tm84618</td>\n",
       "      <td>Taxi Driver</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>A mentally unstable Vietnam War veteran works ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>R</td>\n",
       "      <td>113</td>\n",
       "      <td>['crime', 'drama']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Harvey Keitel</td>\n",
       "      <td>Bill/Phillips Productions</td>\n",
       "      <td>Awards for Taxi Driver</td>\n",
       "      <td>300</td>\n",
       "      <td>soundtrack</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Taxikář (v americkém originále: Taxi Driver) j...</td>\n",
       "      <td>出租车司机 (1976年电影)</td>\n",
       "      <td>Taxi Driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tm127384</td>\n",
       "      <td>Monty Python and the Holy Grail</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>King Arthur, accompanied by his squire, recrui...</td>\n",
       "      <td>1975</td>\n",
       "      <td>PG</td>\n",
       "      <td>91</td>\n",
       "      <td>['comedy', 'fantasy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>John Cleese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>230</td>\n",
       "      <td>Eric Idle</td>\n",
       "      <td>Monty Python a Svatý Grál je britský film kome...</td>\n",
       "      <td>Monty Python e il Sacro Graal</td>\n",
       "      <td>Monty Python and the Holy Grail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tm70993</td>\n",
       "      <td>Life of Brian</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>Brian Cohen is an average young Jewish man, bu...</td>\n",
       "      <td>1979</td>\n",
       "      <td>R</td>\n",
       "      <td>94</td>\n",
       "      <td>['comedy']</td>\n",
       "      <td>['GB']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Terry Gilliam</td>\n",
       "      <td>Python (Monty) Pictures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Eric Idle</td>\n",
       "      <td>La vida de Brian (título original: Life of Bri...</td>\n",
       "      <td>Monty Python : La Vie de Brian</td>\n",
       "      <td>Monty Python's Life of Brian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tm190788</td>\n",
       "      <td>The Exorcist</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>12-year-old Regan MacNeil begins to adapt an e...</td>\n",
       "      <td>1973</td>\n",
       "      <td>R</td>\n",
       "      <td>133</td>\n",
       "      <td>['horror']</td>\n",
       "      <td>['US']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Linda Blair</td>\n",
       "      <td>Hoya Productions</td>\n",
       "      <td>Awards for The Exorcist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>طارد الأرواح الشريرة (بالإنجليزية: The Exorcis...</td>\n",
       "      <td>Der Exorzist</td>\n",
       "      <td>The Exorcist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                title   type  \\\n",
       "0  ts300399  Five Came Back: The Reference Films   SHOW   \n",
       "1   tm84618                          Taxi Driver  MOVIE   \n",
       "2  tm127384      Monty Python and the Holy Grail  MOVIE   \n",
       "3   tm70993                        Life of Brian  MOVIE   \n",
       "4  tm190788                         The Exorcist  MOVIE   \n",
       "\n",
       "                                         description  release_year  \\\n",
       "0  This collection includes 12 World War II-era p...          1945   \n",
       "1  A mentally unstable Vietnam War veteran works ...          1976   \n",
       "2  King Arthur, accompanied by his squire, recrui...          1975   \n",
       "3  Brian Cohen is an average young Jewish man, bu...          1979   \n",
       "4  12-year-old Regan MacNeil begins to adapt an e...          1973   \n",
       "\n",
       "  age_certification  runtime                 genres production_countries  \\\n",
       "0             TV-MA       48      ['documentation']               ['US']   \n",
       "1                 R      113     ['crime', 'drama']               ['US']   \n",
       "2                PG       91  ['comedy', 'fantasy']               ['GB']   \n",
       "3                 R       94             ['comedy']               ['GB']   \n",
       "4                 R      133             ['horror']               ['US']   \n",
       "\n",
       "   seasons  ... new_link_data_http://dbpedia.org/property/starring  \\\n",
       "0      1.0  ...                                                NaN   \n",
       "1      NaN  ...                                      Harvey Keitel   \n",
       "2      NaN  ...                                        John Cleese   \n",
       "3      NaN  ...                                      Terry Gilliam   \n",
       "4      NaN  ...                                        Linda Blair   \n",
       "\n",
       "   new_link_data_http://dbpedia.org/property/studio  \\\n",
       "0                                               NaN   \n",
       "1                         Bill/Phillips Productions   \n",
       "2                                               NaN   \n",
       "3                           Python (Monty) Pictures   \n",
       "4                                  Hoya Productions   \n",
       "\n",
       "   new_link_data_http://dbpedia.org/property/title  \\\n",
       "0                                              NaN   \n",
       "1                           Awards for Taxi Driver   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                          Awards for The Exorcist   \n",
       "\n",
       "   new_link_data_http://dbpedia.org/property/totalWidth  \\\n",
       "0                                                NaN      \n",
       "1                                                300      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   new_link_data_http://dbpedia.org/property/type  \\\n",
       "0                                             NaN   \n",
       "1                                      soundtrack   \n",
       "2                                             NaN   \n",
       "3                                             NaN   \n",
       "4                                             NaN   \n",
       "\n",
       "  new_link_data_http://dbpedia.org/property/width  \\\n",
       "0                                             NaN   \n",
       "1                                             431   \n",
       "2                                             230   \n",
       "3                                            30.0   \n",
       "4                                             NaN   \n",
       "\n",
       "  new_link_data_http://dbpedia.org/property/writers  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                         Eric Idle   \n",
       "3                                         Eric Idle   \n",
       "4                                               NaN   \n",
       "\n",
       "  new_link_data_http://www.w3.org/2000/01/rdf-schema#comment  \\\n",
       "0                                                NaN           \n",
       "1  Taxikář (v americkém originále: Taxi Driver) j...           \n",
       "2  Monty Python a Svatý Grál je britský film kome...           \n",
       "3  La vida de Brian (título original: Life of Bri...           \n",
       "4  طارد الأرواح الشريرة (بالإنجليزية: The Exorcis...           \n",
       "\n",
       "  new_link_data_http://www.w3.org/2000/01/rdf-schema#label  \\\n",
       "0                                                NaN         \n",
       "1                                    出租车司机 (1976年电影)         \n",
       "2                      Monty Python e il Sacro Graal         \n",
       "3                     Monty Python : La Vie de Brian         \n",
       "4                                       Der Exorzist         \n",
       "\n",
       "  new_link_data_http://xmlns.com/foaf/0.1/name  \n",
       "0                                          NaN  \n",
       "1                                  Taxi Driver  \n",
       "2              Monty Python and the Holy Grail  \n",
       "3                 Monty Python's Life of Brian  \n",
       "4                                 The Exorcist  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://kgextension.readthedocs.io/en/latest/source/usage_generators.html#data-properties-generator\n",
    "from kgextension.linking_sklearn import DbpediaLookupLinker\n",
    "from kgextension.generator import data_properties_generator\n",
    "\n",
    "linker = DbpediaLookupLinker(column='title')\n",
    "df_props = linker.fit_transform(titles.head())\n",
    "\n",
    "\n",
    "df_data_properties = data_properties_generator(df_props, 'new_link')\n",
    "df_data_properties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
